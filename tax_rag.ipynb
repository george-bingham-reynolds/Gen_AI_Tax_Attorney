{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3993e8-6b6e-495c-8855-0c142e1aa5ad",
   "metadata": {},
   "source": [
    "NEXT FEW CELLS FOR INSTALLING NECESSARY PACKAGES; HAVE TO RESTART KERNEL AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98366b8a-3997-4e1e-a294-2bfaeaa3ecdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffdc30f-1a6c-4c1e-a720-00902dd68a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Vertex AI LLM SDK\n",
    "! pip install --user --upgrade google-cloud-aiplatform==1.35.0 langchain==0.0.323\n",
    "! pip install typing-inspect==0.8.0\n",
    "! pip install --user typing_extensions==4.5.0\n",
    "\n",
    "# Dependencies required by Unstructured PDF loader\n",
    "! sudo apt -y -qq install tesseract-ocr libtesseract-dev\n",
    "! sudo apt-get -y -qq install poppler-utils\n",
    "! pip install --user unstructured==0.7.5 pdf2image==1.16.3 pytesseract==0.3.10 pdfminer.six==20221105\n",
    "\n",
    "# For Matching Engine integration dependencies (default embeddings)\n",
    "! pip install --user tensorflow_hub==0.13.0 tensorflow_text==2.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481a296-406c-4cd2-ad2b-f892eac6b906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # NOTE THAT TYPING-EXTENSIONS SPECIFICALLY HAS BEEN TRICKY ON VERSIONING\n",
    "# # THE ABOVE CELL SHOULD BE SUFFICIENT, BUT RUN THIS IF NECESSARY\n",
    "# !pip install typing-extensions --upgrade\n",
    "# # THIS GAVE VERSION Version: 4.5.0 IF YOU HAVE ISSUES LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08bcee-dd01-4505-a30b-1138e7132280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866a782-e59e-4124-9d3a-5065ad2382e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GET HELPER FUNCTIONS NEEDED FOR MATCHING ENGINE LATER IN NB\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"utils\"):\n",
    "    os.makedirs(\"utils\")\n",
    "\n",
    "url_prefix = \"https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/use-cases/document-qa/utils\"\n",
    "files = [\"__init__.py\", \"matching_engine.py\", \"matching_engine_utils.py\"]\n",
    "\n",
    "for fname in files:\n",
    "    urllib.request.urlretrieve(f\"{url_prefix}/{fname}\", filename=f\"utils/{fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e71ea7-6a1f-4e99-b631-c25363f7ef99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Utils\n",
    "import time\n",
    "import uuid\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import vertexai\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "\n",
    "# LangChain\n",
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "from typing_extensions import TypeAlias\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import GCSDirectoryLoader\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Import custom Matching Engine packages\n",
    "from utils.matching_engine import MatchingEngine\n",
    "from utils.matching_engine_utils import MatchingEngineUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc04b9-6f1a-4cbc-837b-bb9a987e10f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\" #NOTE THAT YOU'LL WANT SAME REGION FOR INSTANCE, BUCKETS, ENDPOINTS, ETC.\n",
    "\n",
    "# INIT VERTEX AI SDK \n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e46c75-a813-4abf-91f9-98f8845ced31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add6c828-096a-47d3-a583-7ec4af4535c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEXT MODEL INTEGRATED WITH LANGCHAIN \n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0.2,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# EMBEDDINGS API INTEGRATED WITH LANGCHAIN \n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3938d7df-571b-4967-bd40-7a8be6325f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE TO SELF - ME = MATCHING ENGINE\n",
    "ME_REGION = \"us-central1\" # NEEDS TO ALIGN WITH REGION VAR\n",
    "ME_INDEX_NAME = \"tax-rag-me-index-test\"  # REPLACE WITH YOUR OWN NAMING CONVENTION\n",
    "ME_EMBEDDING_DIR = \"tax-rag-me-bucket-test\"  # SAME\n",
    "ME_DIMENSIONS = 768  # WHEN USING VERTEX PaLM EMBEDDING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786771e7-12e3-4c10-ae1d-2f3cfe82b404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CREATE BUCKET IF YOU HAVEN'T ALREADY\n",
    "!gsutil mb -l {REGION} gs://{ME_EMBEDDING_DIR} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914cf285-17be-4898-b105-9c4fb21990cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CREATE A DUMMY EMBEDDINGS FILE TO INITIALIZE WHEN CREATING THE INDEX\n",
    "\n",
    "# DUMMY EMBEDDING\n",
    "init_embedding = {\"id\": str(uuid.uuid4()), \"embedding\": list(np.zeros(ME_DIMENSIONS))}\n",
    "\n",
    "# DUMP EMBEDDING TO LOCAL FILE \n",
    "with open(\"embeddings_0.json\", \"w\") as f:\n",
    "    json.dump(init_embedding, f)\n",
    "\n",
    "# write embedding to Cloud Storage\n",
    "! set -x && gsutil cp embeddings_0.json gs://{ME_EMBEDDING_DIR}/init_index/embeddings_0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607fe8dc-f3b7-4311-b102-23cf7282188a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CREATE MATCHING ENGINE VAR\n",
    "mengine = MatchingEngineUtils(PROJECT_ID, ME_REGION, ME_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a0918-9fbd-4010-a5d2-744449d53a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GET INDEX\n",
    "index = mengine.create_index(\n",
    "    embedding_gcs_uri=f\"gs://{ME_EMBEDDING_DIR}/init_index\",\n",
    "    dimensions=ME_DIMENSIONS,\n",
    "    index_update_method=\"streaming\",\n",
    "    index_algorithm=\"tree-ah\",\n",
    ")\n",
    "if index:\n",
    "    print(index.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd379c4-6fbb-4338-8de6-f832245d26dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AND ENDPOINT\n",
    "index_endpoint = mengine.deploy_index()\n",
    "if index_endpoint:\n",
    "    print(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
    "    print(\n",
    "        f\"Index endpoint public domain name: {index_endpoint.public_endpoint_domain_name}\"\n",
    "    )\n",
    "    print(\"Deployed indexes on the index endpoint:\")\n",
    "    for d in index_endpoint.deployed_indexes:\n",
    "        print(f\"    {d.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2045a-4c55-443a-ad5c-ee0ebe803c47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# POINT TO BUCKET WITH THE PDFS YOU WANT FOR SEMANTIC SEARCH LATER\n",
    "PDF_BUCKET = \"irs_written_determinations_test\" # REPLACE WITH YOUR OWN BUCKET\n",
    "BUCKET = 'gs://irs_written_determinations_test/' # HAD ISSUES WITH NEEDING GS SOMETIMES AND NOT OTHER TIMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec1821-3710-459f-98b3-900ff56dfe24",
   "metadata": {},
   "source": [
    "EXTREMELY IMPORTANT NOTE READ BEFORE MOVING FORARD!!\n",
    "\n",
    "TLDR; NEED TO USE NANO TO EDIT ~/.local/lib/python3.10/site-packages/unstructured/partition/strategies.py \\\n",
    "ADD THE SNIPPET if sum(1 for _ in PDFPage.get_pages(fp, check_extractable=True)) > 0: BEFORE THE PART THAT EXECUTES THE NEXT() STATEMENT AT THE CODE BLOCK STARTING WITH def _fp_is_extractable(fp):\n",
    "\n",
    "documents = loader.load() LINE USED TO MAKE THE BELOW CELL BARF; CHECKED PDFs ONE BY ONE AND FOUND 0303021.pdf WAS THE FIRST CULPRIT. THERE WAS NO REAL ERROR, IT JUST SAID \"StopIteration: \", WHICH I GOOGLED AROUND AND SAW HAPPENS WHEN AN ITERATOR OR GENERATOR IS EXHAUSTED. \n",
    "\n",
    "ERROR NOTES SHOW THAT THIS HAPPENED FROM A LINE OF CODE next(PDFPage.get_pages(fp, check_extractable=True)) IN ~/.local/lib/python3.10/site-packages/unstructured/partition/strategies.py. NOTE THAT PDFPage.get_pages(fp, check_extractable=True) RETURNS A GENERATOR, SO THIS MAKES SENSE.\n",
    "\n",
    "THIS IMPLIES THAT THE ISSUE IS THAT FOR THIS PDF, THE PDFPage.get_pages FINDS NO PAGES (I ASSUME THIS GENERATOR CONTAINS PAGES BUT IDK FOR SURE) FOR THAT FILE PATH; I THINK THIS ERROR IS BIZARRE BECAUSE THE PDF CLEARLY EXISTS AND HAS PAGES. LATER ON I WANT TO LOOK INTO THIS, BUT FOR NOW I THINK IT'S ENOUGH TO JUST EXCLUDE FILES THAT CAUSE THIS PROBLEM. SO, MY WORKAROUND IS TO GO INTO THE FILE AND ADD THE CODE if sum(1 for _ in PDFPage.get_pages(fp, check_extractable=True)) > 0: BEFORE THE PART THAT EXECUTES THE NEXT() STATEMENT SO THAT WE ONLY DEAL WITH FILES THAT THIS FUNCTION CAN HANDLE. AGAIN, AT A LATER POINT I'LL TRY TO FIGURE OUT A FIX THAT DOESN'T TOSS GOOD FILES; FOR NOW HOPEFULLY THIS IS AN UNCOMMON ERROR AND COSTS US FAIRLY LITTLE.\n",
    "\n",
    "IF YOU HAVE ISSUES IN THE FUTURE KNOW THAT THIS WAS PART OF THE CODE BLOCK STARTING WITH def _fp_is_extractable(fp):\n",
    "\n",
    "ADDENDUM - MY FIX NOT ONLY STOPPED IT FROM BARFING, BUT THE FILE WASN'T EVEN TOSSED; RAN CELLS WITH JUST THAT ONE PDF AS THE INPUT AND THE DOWNSTREAM VECTOR SEARCH RETURNED CHUNKS FROM THE DOC RELATED TO THE Q; WE GUCCI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520e353-d65b-4d0d-bd3e-d9e9fe459058",
   "metadata": {
    "tags": []
   },
   "source": [
    "NOTE THAT YOU HAVE TO GIVE STORAGE ADMIN ACCESS TO THE SERVICE ACCOUNT ENDING IN compute@developer.gserviceaccount.com TO THE BUCKETS FOR THIS TO WORK; YOU CAN DO THIS IN THE IAM TAB OF THE GCP PAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bbbafc-53ce-4d52-b448-39fb14fe2d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INGEST PDF FILES \n",
    "\n",
    "print(f\"Processing documents from {PDF_BUCKET}\")\n",
    "loader = GCSDirectoryLoader(\n",
    "    project_name=PROJECT_ID, bucket=PDF_BUCKET#, prefix=folder_prefix\n",
    ")\n",
    "documents = loader.load()\n",
    "# ADD DOC NAME/SOURCE TO METADATA \n",
    "for document in documents:\n",
    "    doc_md = document.metadata\n",
    "    document_name = doc_md[\"source\"].split(\"/\")[-1]\n",
    "    # GET DOC SOURCE FROM DOC LOADER \n",
    "    doc_source_prefix = \"/\".join(PDF_BUCKET.split(\"/\")[:3])\n",
    "    doc_source_suffix = \"/\".join(doc_md[\"source\"].split(\"/\")[4:-1])\n",
    "    source = f\"{doc_source_prefix}/{doc_source_suffix}\"\n",
    "    document.metadata = {\"source\": source, \"document_name\": document_name}\n",
    "\n",
    "print(f\"# of documents loaded (pre-chunking) = {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e4b37b-aea3-45c5-8e13-7fb457f83fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CHECK METADATA\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59befe8a-763f-4442-93c5-8f7e389f837a",
   "metadata": {},
   "source": [
    "IF YOU NEED TO RESTART THE KERNEL AT ANY POINT (E.G. WANT TO SHUT DOWN INSTANCE WHILE NOT WORKING IN NOTEBOOKS) IT IS SUPER USEFUL TO HAVE THE DOCUMENTS LIST AVAILABLE WITHOUT HAVING TO RE-INGEST THEM. BELOW CELL STORES THE VAR FOR YOU AND THEN THE ONE BELOW WILL READ IT IN WITHOUT HAVING TO RE-RUN ABOVE INGESTION CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4364a66-cc90-4406-ac16-0b8f79f2f4c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df7adfe-5b51-47d8-808a-80f7d7961ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %store -r documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a412b-daf5-48b3-a49b-fafebb28ac57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SPLIT DOCS INTO CHUNKS FOR AFFORDABLE SEARCH \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# ADD CHUNK NUM TO METADAT\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"chunk\"] = idx\n",
    "\n",
    "print(f\"# of documents = {len(doc_splits)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560f536-78b2-4264-a53d-6a8b2f56665e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c89978-9d01-40c3-91fa-5d8e4603f908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store doc_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42dbcc2-3442-417f-9736-f8c497994b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %store -r doc_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6312c-5709-4901-80f8-76636027e910",
   "metadata": {},
   "source": [
    "CONFIGURE MATCHING ENGINE AS VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d2c4c-85d3-419d-ae1f-d92ebd2a14be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()\n",
    "print(f\"ME_INDEX_ID={ME_INDEX_ID}\")\n",
    "print(f\"ME_INDEX_ENDPOINT_ID={ME_INDEX_ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded9825-54b4-4822-822c-b62092b992b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store ME_INDEX_ID ME_INDEX_ENDPOINT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c47676-6eb4-4506-858f-16b00e280765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INIT ME VECTOR STORE W/TEXT EMBEDDING MODEL\n",
    "\n",
    "me = MatchingEngine.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=ME_REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_DIR}\".split(\"/\")[2],\n",
    "    embedding=embeddings,\n",
    "    index_id=ME_INDEX_ID,\n",
    "    endpoint_id=ME_INDEX_ENDPOINT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb53fa-5f6b-428b-9fc4-d360e131b808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STORE DOCS AS EMBEDDINGS IN MATCHING ENGINE INDEX \n",
    "# LIMITED API RATE MAY MEAN THIS TAKES A WHILE  \n",
    "texts = [doc.page_content for doc in doc_splits]\n",
    "metadatas = [\n",
    "    [\n",
    "        {\"namespace\": \"source\", \"allow_list\": [doc.metadata[\"source\"]]},\n",
    "        {\"namespace\": \"document_name\", \"allow_list\": [doc.metadata[\"document_name\"]]},\n",
    "        {\"namespace\": \"chunk\", \"allow_list\": [str(doc.metadata[\"chunk\"])]},\n",
    "    ]\n",
    "    for doc in doc_splits\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd58c06-c6c7-4695-9dc1-cf90396b9c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store texts\n",
    "%store metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6766aa-010d-4fd2-93b7-5bbe6e4c930e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %store -r texts\n",
    "# %store -r metadatas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c3ff7-5752-44ae-a073-7f4a40b37248",
   "metadata": {},
   "source": [
    "A FEW NOTES ON ADDING EMBEDDINGS TO VECTORE STORE:\n",
    "\n",
    "1 - THIS TAKES A VERY LONG TIME (IT'S THE LONGEST PART OF THIS PROCESS)\\\n",
    "2 - I'VE SEEN MANY INSTANCES OF THE KERNEL DYING OR CONNECTION BEING INTERRUPTED BEFORE THE PROCESS FINISHES\\\n",
    "3 - ADDING THE EMBEDDINGS 100 AT A TIME IS MEANT TO HELP MITIGATE THIS - EVEN IF SOMETHING HAPPENS TO INTERRUPT, THE ONES ALREADY ADDED WILL BE THERE MOVING FORWARD\\\n",
    "4 - THE GSUTIL CELL GIVES THE NUMBER OF EMBEDDINGS ADDED TO THE EMBEDDING DIRECTORY; IN THE EVENT THAT SOMETHING INTERRUPTS THE ADDITIONS, RUN THIS CELL TO SEE HOW FAR IT MADE IT THROUGH THE DOCS AND THEN RE-RUN THE FOR LOOP STARTING AT THE REQUISITE POINT IN THE TEXTS/METADATAS LISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f80947-8365-4016-ab01-d79d61802676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ADD EMBEDDINGS TO THE VECTOR STORE\n",
    "for i in np.arange(0, len(metadatas), 100):\n",
    "    doc_ids = me.add_texts(texts=texts[i - 100:i], metadatas=metadatas[i - 100:i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a37019-caba-40d0-8916-9837519324f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil du gs://{ME_EMBEDDING_DIR} | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df4b73-1189-4ede-9644-c60d194776bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SPOT CHECK LAST ONE LOOKS CORRECT\n",
    "doc_ids[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1d8e9-5d1b-42b7-aad3-2d1069ad4e34",
   "metadata": {},
   "source": [
    "CLEANUP/DELETING RESOURCES - RUNNING THESE WILL DELETE EVERYTHING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ebc0f-ef2f-4586-9a72-5d94d4de52ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CLEANUP_RESOURCES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6fcf4-3579-49b3-a192-5561ec3da57e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()\n",
    "# print(f\"ME_INDEX_ID={ME_INDEX_ID}\")\n",
    "# print(f\"ME_INDEX_ENDPOINT_ID={ME_INDEX_ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b975345f-e0da-46b3-b266-80071bc37d14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if CLEANUP_RESOURCES and \"mengine\" in globals():\n",
    "#     print(\n",
    "#         f\"Undeploying all indexes and deleting the index endpoint {ME_INDEX_ENDPOINT_ID}\"\n",
    "#     )\n",
    "#     mengine.delete_index_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1624d1-3c79-484f-b851-61701857082e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if CLEANUP_RESOURCES and \"mengine\" in globals():\n",
    "#     print(f\"Deleting the index {ME_INDEX_ID}\")\n",
    "#     mengine.delete_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858f94c-0ea0-4812-bd5b-694d5b89d94d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if CLEANUP_RESOURCES and \"ME_EMBEDDING_DIR\" in globals():\n",
    "#     print(f\"Deleting contents from the Cloud Storage bucket {ME_EMBEDDING_DIR}\")\n",
    "#     ME_EMBEDDING_BUCKET = \"/\".join(ME_EMBEDDING_DIR.split(\"/\")[:3])\n",
    "\n",
    "#     shell_output = ! gsutil du -ash gs://$ME_EMBEDDING_BUCKET\n",
    "#     print(shell_output)\n",
    "#     print(\n",
    "#         f\"Size of the bucket {ME_EMBEDDING_BUCKET} before deleting = {' '.join(shell_output[0].split()[:2])}\"\n",
    "#     )\n",
    "\n",
    "#     # uncomment below line to delete contents of the bucket\n",
    "#     ! gsutil -m rm -r gs://$ME_EMBEDDING_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1bcf33-9bec-45cb-bee2-cf57aa1fc181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
